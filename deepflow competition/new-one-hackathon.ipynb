{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport re \nimport json\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-07T00:16:03.532690Z","iopub.execute_input":"2023-05-07T00:16:03.533351Z","iopub.status.idle":"2023-05-07T00:16:03.539535Z","shell.execute_reply.started":"2023-05-07T00:16:03.533310Z","shell.execute_reply":"2023-05-07T00:16:03.538604Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def preprocess(df):\n    # Replace single quotes with double quotes in \"random load mesures\" column\n    df[\"random load mesures\"] = df[\"random load mesures\"].str.replace(\"'\",'\"')\n    \n    # Remove prefix from building and file columns and convert to int\n    df['building'] = df['building'].str.replace(\"Building_\",\"\").astype(int)\n    df['File'] = df['File'].str.replace(\"Shape0_\",\"\").astype(int)\n    \n    # Extract WWR feature using regular expression\n    pattern = r'\\d+\\.\\d+'\n    wwr = df['WWR'].apply(lambda x: max([float(m) for m in re.findall(pattern, x)]))\n    df['WWR'] = wwr\n    \n    # Extract Cooling and Lights features using JSON\n    features = df['random load mesures'].apply(lambda x: json.loads(x))\n    cooling = features.apply(lambda x: float(x['Cooling'].replace(\":C\",\"\")))\n    lights = features.apply(lambda x: float(x['Lights'].replace(\":C\",\"\")))\n    df['Coolings'] = cooling\n    df['Lights'] = lights\n    # Load the dataset into a pandas DataFrame\n\n    # Create a new feature: Overall Energy Consumption\n    df['Overall Energy Consumption'] = df['Operating Hours'] * df['EUI']\n\n    # Create a new feature: Impact of Lighting on Heat Load\n    df['Lighting Impact on Heat Load'] = df['Lights'] * df['Light Heat Gain']\n\n    # Create a new feature: Energy Efficiency in relation to Building Size\n    df['Energy Efficiency'] = df['EUI'] / df['Number of Floors']\n\n    # Create a new feature: Rate of Heat Addition through Lighting\n    df['Lighting Heat Addition Rate'] = df['Light Heat Gain'] / df['Operating Hours']\n\n    # Create a new feature: Logarithm of EUI\n    df['Logarithm EUI'] = np.log(df['EUI'])\n\n    # Create a new feature: Building Size Categories\n    bins = [0, 5, 10, np.inf]\n    #labels = ['Small', 'Medium', 'Large']\n    #%df['Building Size Category'] = pd.cut(df['Number of Floors'], bins=bins, labels=labels)\n\n    # Create interaction features\n    interaction_features = ['Overall Energy Consumption', 'Lighting Impact on Heat Load', 'Energy Efficiency']\n    for feature1 in interaction_features:\n        for feature2 in interaction_features:\n            if feature1 != feature2:\n                interaction_feature_name = f'{feature1} x {feature2}'\n                df[interaction_feature_name] = df[feature1] * df[feature2]\n\n    # Save the updated dataset\n    #df.to_csv('improved_dataset.csv', index=False)\n\n    # Drop unnecessary columns\n    df = df.drop(['random load mesures', 'Permeability','File','building'], axis=1)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:16:03.541287Z","iopub.execute_input":"2023-05-07T00:16:03.541683Z","iopub.status.idle":"2023-05-07T00:16:03.556902Z","shell.execute_reply.started":"2023-05-07T00:16:03.541646Z","shell.execute_reply":"2023-05-07T00:16:03.555898Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/traintrain/Train.csv\")\ntest = pd.read_csv(\"/kaggle/input/testing/Test (1).csv\")\n\n# Preprocess train and test data\ntrain = preprocess(train)\ntest = preprocess(test)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:16:03.558248Z","iopub.execute_input":"2023-05-07T00:16:03.559156Z","iopub.status.idle":"2023-05-07T00:16:05.143173Z","shell.execute_reply.started":"2023-05-07T00:16:03.559122Z","shell.execute_reply":"2023-05-07T00:16:05.142005Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Define the features and target\nX = train.drop(['Operational Energy'], axis=1)\ny = train['Operational Energy']\n\n# Set up the KFold object\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# Initialize the list to store the mean squared errors (MSEs)\nmse_list = []","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:16:05.145282Z","iopub.execute_input":"2023-05-07T00:16:05.145745Z","iopub.status.idle":"2023-05-07T00:16:05.160066Z","shell.execute_reply.started":"2023-05-07T00:16:05.145680Z","shell.execute_reply":"2023-05-07T00:16:05.158771Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for train_index, val_index in kf.split(X):\n    \n    # Split the data into training and validation sets\n    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n    \n    # Scale the data using StandardScaler\n    scaler = StandardScaler()\n    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n    X_val_fold_scaled = scaler.transform(X_val_fold)\n    \n    # Apply PCA to reduce dimensionality\n    pca = PCA(n_components=10)\n    X_train_fold_pca = pca.fit_transform(X_train_fold_scaled)\n    X_val_fold_pca = pca.transform(X_val_fold_scaled)\n    \n    # Train the model\n    model.fit(X_train_fold_pca, y_train_fold)\n    \n    # Make predictions on the validation set and calculate MSE\n    y_pred = model.predict(X_val_fold_pca)\n    mse = np.mean((y_val_fold - y_pred)**2)\n    \n    # Append the MSE to the list\n    mse_list.append(mse)\n    \n# Print the mean of the MSEs\nprint(\"Mean MSE:\", np.mean(mse_list))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:16:05.161906Z","iopub.execute_input":"2023-05-07T00:16:05.162391Z","iopub.status.idle":"2023-05-07T00:20:20.910981Z","shell.execute_reply.started":"2023-05-07T00:16:05.162346Z","shell.execute_reply":"2023-05-07T00:20:20.909938Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Mean MSE: 680888280.0198196\n","output_type":"stream"}]},{"cell_type":"code","source":"importances = model.feature_importances_\n\n# Sort feature importances in descending order\nsorted_importances = sorted(zip(importances, X.columns), reverse=True)\n\n# Print the feature importances\nfor importance, feature in sorted_importances:\n    print(f\"{feature}: {importance}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:20:20.913100Z","iopub.execute_input":"2023-05-07T00:20:20.914172Z","iopub.status.idle":"2023-05-07T00:20:21.006680Z","shell.execute_reply.started":"2023-05-07T00:20:20.914137Z","shell.execute_reply":"2023-05-07T00:20:21.005505Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"EUI: 0.3599299350382553\nCooling Setpoint: 0.2575559166030452\nCooling COP: 0.13696919245982667\nOperating Hours: 0.09945544782219912\nWWR: 0.05492314417299849\nEquipment Heat Gain: 0.020734228438313615\nInternal Floor Rt: 0.01876654600654251\nInfiltration: 0.018609734430185383\nGround Floor Rt: 0.0172027538294306\nInternal Wall Rt: 0.015853101199202982\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load train and test data\ntrain = pd.read_csv(\"/kaggle/input/traintrain/Train.csv\")\ntest = pd.read_csv(\"/kaggle/input/testing/Test (1).csv\")\n\n# Preprocess train and test data\ntrain = preprocess(train)\ntest = preprocess(test)\n\n# Initialize the model\nmodel = xgb.XGBRegressor(objective='reg:squarederror')\n\n# Define the features and target\nX = train.drop(['Operational Energy'], axis=1)\ny = train['Operational Energy']\n\n# Feature selection using XGBoost\nmodel.fit(X, y)\nimportance_scores = model.feature_importances_\nfeature_importances = dict(zip(X.columns, importance_scores))\ntop_5_features = sorted(feature_importances, key=feature_importances.get, reverse=True)[:5]\n\n# Select only the top 5 features\nX = X[top_5_features]\ntest = test[top_5_features]\n\n# Set up the KFold object\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# Initialize the list to store the mean squared errors (MSEs)\nmse_list = []\n\n# Loop over the folds\nfor train_index, val_index in kf.split(X):\n    \n    # Split the data into training and validation sets\n    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n\n    # Scale the features\n    scaler = StandardScaler()\n    X_train_fold = scaler.fit_transform(X_train_fold)\n    X_val_fold = scaler.transform(X_val_fold)\n    \n    # Reduce dimensionality using PCA\n    pca = PCA(n_components=2)\n    X_train_fold = pca.fit_transform(X_train_fold)\n    X_val_fold = pca.transform(X_val_fold)\n\n    # Train the model and predict on the validation set\n    model.fit(X_train_fold, y_train_fold)\n    y_pred = model.predict(X_val_fold)\n\n    # Compute the mean squared error\n    mse = np.mean((y_val_fold - y_pred)**2)\n    mse_list.append(mse)\n\n# Compute the average mean squared error over the folds\navg_mse = np.mean(mse_list)\nprint(\"Average MSE:\", avg_mse)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:20:21.008228Z","iopub.execute_input":"2023-05-07T00:20:21.008792Z","iopub.status.idle":"2023-05-07T00:20:51.230300Z","shell.execute_reply.started":"2023-05-07T00:20:21.008759Z","shell.execute_reply":"2023-05-07T00:20:51.229478Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Average MSE: 659031181.7634231\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pris1=scaler.fit_transform(test)\ny_pris1=pca.transform(y_pris1)\ny_pris1 = model.predict(y_pris1)\ny_pris1","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:20:51.234159Z","iopub.execute_input":"2023-05-07T00:20:51.236181Z","iopub.status.idle":"2023-05-07T00:20:51.276780Z","shell.execute_reply.started":"2023-05-07T00:20:51.236144Z","shell.execute_reply":"2023-05-07T00:20:51.275805Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"array([ 90313.336,  85569.72 ,  84374.46 , ..., 248714.2  ,  71751.98 ,\n       202789.73 ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"csv_file=pd.read_csv(\"/kaggle/input/simple/SampleSubmission.csv\")\ncsv_file['Operational Energy']=y_pris1\ncsv_file.to_csv(\"simplesimple100+.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T00:21:05.061765Z","iopub.execute_input":"2023-05-07T00:21:05.062136Z","iopub.status.idle":"2023-05-07T00:21:05.141078Z","shell.execute_reply.started":"2023-05-07T00:21:05.062109Z","shell.execute_reply":"2023-05-07T00:21:05.139957Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}