{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport re \nimport json\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom scipy import stats\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-08T10:13:15.295559Z","iopub.execute_input":"2023-05-08T10:13:15.296276Z","iopub.status.idle":"2023-05-08T10:13:15.303019Z","shell.execute_reply.started":"2023-05-08T10:13:15.296240Z","shell.execute_reply":"2023-05-08T10:13:15.301509Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def preprocess(data):\n    # Replace single quotes with double quotes in \"random load mesures\" column\n    #data[\"random load measures\"] = data[\"random load measures\"].str.replace(\"'\",'\"')\n    data['building'] = le.fit_transform(data['building'])\n    data['Town'] = le.fit_transform(data['Town'])\n    data['Total Wall Area'] = data['Wall Rt'] * data['Height'] * data['Number of Floors']\n    #data['Total Window Area'] = data['Windows Rt'] * data['Height'] * data['WWR'] * data['Number of Floors']\n    data['Total Floor Area'] = data['Ground Floor Rt'] * data['Height']\n    data['Total Roof Area'] = data['Roof Rt'] * data['Height']\n    data['Total Internal Wall Area'] = data['Internal Wall Rt'] * data['Height'] * data['Number of Floors']\n    data['Total Internal Floor Area'] = data['Internal Floor Rt'] * data['Height']\n    data['Overall Energy Consumption'] = data['Operating Hours'] * data['EUI']\n\n    \n    pattern = r'\\d+\\.\\d+'\n    wwr = data['WWR'].apply(lambda x: max([float(m) for m in re.findall(pattern, x)]))\n    data['WWR'] = wwr\n    # assuming 'File' contains unique file names for each building shape, it can be removed\n    \n    \n    features = data['random load mesures'].apply(lambda x: json.loads(x))\n    cooling = features.apply(lambda x: float(x['Cooling'].replace(\":C\",\"\")))\n    lights = features.apply(lambda x: float(x['Lights'].replace(\":C\",\"\")))\n    data['Coolings'] = cooling\n    data['Lights'] = lights\n    \n    data['Lighting Impact on Heat Load'] = data['Lights'] * data['Light Heat Gain']\n    data['Energy Efficiency'] = data['EUI'] / data['Number of Floors']\n    data['Lighting Heat Addition Rate'] = data['Light Heat Gain'] / data['Operating Hours']\n    \n\n    scaler = StandardScaler()\n    data[['Cooling Setpoint','Coolings','Lights', 'EUI', 'Cooling COP', 'Operating Hours', 'WWR',      'Equipment Heat Gain', 'Internal Wall Rt', 'Internal Floor Rt', 'Infiltration','Ground Floor Rt', 'Number of Floors', 'Occupancy', 'Light Heat Gain', 'Windows Rt',      'Height', 'Heating COP', 'Heating Setpoint', 'Wall Rt', 'Start Time', 'windows g-value',      'Roof Rt', 'Boiler Efficiency', 'Internal Mass', 'Permeability', 'Total Floors Area',]] = scaler.fit_transform(data[['Cooling Setpoint','Coolings','Lights', 'EUI', 'Cooling COP', 'Operating Hours', 'WWR',                                                              'Equipment Heat Gain', 'Internal Wall Rt', 'Internal Floor Rt', 'Infiltration',                                                              'Ground Floor Rt', 'Number of Floors', 'Occupancy', 'Light Heat Gain', 'Windows Rt',                                                              'Height', 'Heating COP', 'Heating Setpoint', 'Wall Rt', 'Start Time', 'windows g-value',                                                              'Roof Rt', 'Boiler Efficiency', 'Internal Mass', 'Permeability', 'Total Floors Area']])\n    \n    interaction_features = ['Overall Energy Consumption', 'Lighting Impact on Heat Load', 'Energy Efficiency','Lighting Heat Addition Rate']\n    for feature1 in interaction_features:\n        for feature2 in interaction_features:\n            if feature1 != feature2:\n                interaction_feature_name = f'{feature1} x {feature2}'\n                data[interaction_feature_name] = data[feature1] * data[feature2]\n\n    # assuming we want to select the top 10 features with the highest F-test score\n    selector = SelectKBest(f_regression, k=10)\n    data = data.drop(['random load mesures', 'Permeability','File','building'], axis=1)\n    #data = selector.fit_transform(data.drop('Operational Energy', axis=1), data['Operational Energy'])\n    # assume your data is stored in a NumPy array called `data`\n    for col in data.columns:\n        if col == 'Operational Energy':\n            continue  # skip target variable\n        median = np.median(data[col])\n        q1 = np.percentile(data[col], 25)\n        q3 = np.percentile(data[col], 75)\n        iqr = q3 - q1\n        upper_bound = q3 + 1.5 * iqr\n        lower_bound = q1 - 1.5 * iqr\n        data.loc[data[col] > upper_bound, col] = median\n        data.loc[data[col] < lower_bound, col] = median\n\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T10:13:15.622221Z","iopub.execute_input":"2023-05-08T10:13:15.622658Z","iopub.status.idle":"2023-05-08T10:13:15.642303Z","shell.execute_reply.started":"2023-05-08T10:13:15.622623Z","shell.execute_reply":"2023-05-08T10:13:15.640864Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/traintrain/Train.csv\")\ntest = pd.read_csv(\"/kaggle/input/testtest/Test (2).csv\")\ncsv_file = pd.DataFrame()\ncsv_file['submission id'] = test['building'] + '_Town_' + test['Town'].astype(str)\n\ncsv_file.head()\ncsv_file1 = pd.DataFrame()\ncsv_file1['submission id'] = test['building'] + '_Town_' + test['Town'].astype(str)\n\ncsv_file1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Preprocess train and test data\ntrain = preprocess(train)\nprint(test.shape)\ntest = preprocess(test)\nprint(test.shape)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#diff_cols = set(test.columns).difference(X.columns)\n#print(test['Operational Energy'])\n#test=test.drop('Operational Energy',axis=1)\ntest.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n\ncatboost_model = CatBoostRegressor()\n\nX = train.drop(['Operational Energy'], axis=1)\n#test=test.drop(['Operational Energy'])\ny = train['Operational Energy']\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# Initialize the list to store the mean squared errors (MSEs)\nmse_list = []\ntest.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_index, val_index in kf.split(X):\n    \n    # Split the data into training and validation sets\n    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n    \n    # Scale the data using StandardScaler\n    scaler = StandardScaler()\n    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n    X_val_fold_scaled = scaler.transform(X_val_fold)\n    \n    # Apply PCA to reduce dimensionality\n    pca = PCA(n_components=10)\n    X_train_fold_pca = pca.fit_transform(X_train_fold_scaled)\n    X_val_fold_pca = pca.transform(X_val_fold_scaled)\n    \n    # Train the model\n    catboost_model.fit(X_train_fold_pca, y_train_fold)\n    \n    # Make predictions on the validation set and calculate MSE\n    y_pred = catboost_model.predict(X_val_fold_pca)\n    mse = np.mean((y_val_fold - y_pred)**2)\n    \n    # Append the MSE to the list\n    mse_list.append(mse)\n    \n# Print the mean of the MSEs\nprint(\"Mean MSE:\", np.mean(mse_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_4b=scaler.fit_transform(test)\ny_4b=pca.transform(y_4b)\ny_4b = catboost_model.predict(y_4b)\ny_4b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file['Operational Energy']=y_4b\ncsv_file.to_csv(\"simplesimple1235.csv\",index=False)\ncsv_file.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = catboost_model.feature_importances_\n\n# Sort feature importances in descending order\nsorted_importances = sorted(zip(importances, X.columns), reverse=True)\n\n# Print the feature importances\nfor importance, feature in sorted_importances:\n    print(f\"{feature}: {importance}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_scores = catboost_model.feature_importances_\nfeature_importances = dict(zip(X.columns, importance_scores))\ntop_8_features = sorted(feature_importances, key=feature_importances.get, reverse=True)[:8]\n\n# Select only the top 5 features\nX = X[top_8_features]\ntest = test[top_8_features]\n\n# Set up the KFold object\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# Initialize the list to store the mean squared errors (MSEs)\nmse_list = []\n\n# Loop over the folds\nfor train_index, val_index in kf.split(X):\n    \n    # Split the data into training and validation sets\n    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n\n    # Scale the features\n    scaler = StandardScaler()\n    X_train_fold = scaler.fit_transform(X_train_fold)\n    X_val_fold = scaler.transform(X_val_fold)\n    \n    # Reduce dimensionality using PCA\n    pca = PCA(n_components=2)\n    X_train_fold = pca.fit_transform(X_train_fold)\n    X_val_fold = pca.transform(X_val_fold)\n\n    # Train the model and predict on the validation set\n    catboost_model.fit(X_train_fold, y_train_fold)\n    y_pred = catboost_model.predict(X_val_fold)\n\n    # Compute the mean squared error\n    mse = np.mean((y_val_fold - y_pred)**2)\n    mse_list.append(mse)\n\n# Compute the average mean squared error over the folds\navg_mse = np.mean(mse_list)\nprint(\"Average MSE:\", avg_mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test=scaler.fit_transform(test)\ny_test_pca=pca.transform(y_test)\ny_test_last = catboost_model.predict(y_test_pca)\ny_test_last.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file1['Operational Energy']=y_test_last\ncsv_file1.to_csv(\"top10.csv\",index=False)\ncsv_file1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# load the dataset\ndf = pd.read_csv('/kaggle/input/traintrain/Train.csv')\ntest = pd.read_csv('/kaggle/input/testtest/Test (2).csv')\n\n# preprocess the data\ntrain = preprocess(train)\ntest = preprocess(test)\n\n# select features and target variable\n\nX = train.drop(['Operational Energy'], axis=1)\n#test=test.drop(['Operational Energy'])\ny = train['Operational Energy']\n\n# split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# create polynomial features of degree 2\npoly_features = PolynomialFeatures(degree=2)\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.transform(X_test)\ntest_poly = poly_features.transform(test)\n\n# apply L2 regularization using Ridge regression\nalpha = 0.1  # regularization strength\nmodel = Ridge(alpha=alpha)\nmodel.fit(X_train_poly, y_train)\n\n# make predictions on the test set\ny_pred = model.predict(X_test_poly)\n\n# evaluate the model's performance\nmse = mean_squared_error(y_pred, y_test)\nprint(\"Mean squared error:\", mse)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T10:13:40.331094Z","iopub.execute_input":"2023-05-08T10:13:40.331731Z","iopub.status.idle":"2023-05-08T10:13:41.486825Z","shell.execute_reply.started":"2023-05-08T10:13:40.331697Z","shell.execute_reply":"2023-05-08T10:13:41.485102Z"},"trusted":true},"execution_count":43,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'building'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/testtest/Test (2).csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# preprocess the data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m test \u001b[38;5;241m=\u001b[39m preprocess(test)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# select features and target variable\u001b[39;00m\n","Cell \u001b[0;32mIn[41], line 4\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(data):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Replace single quotes with double quotes in \"random load mesures\" column\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#data[\"random load measures\"] = data[\"random load measures\"].str.replace(\"'\",'\"')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbuilding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      5\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTown\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTown\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Wall Area\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWall Rt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Floors\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'building'"],"ename":"KeyError","evalue":"'building'","output_type":"error"}]},{"cell_type":"code","source":"csv_file2 = pd.DataFrame()\ncsv_file2['submission id'] = test['building'] + '_Town_' + test['Town'].astype(str)\ncsv_file2['Operational Energy']=y_test_last\ncsv_file2.to_csv(\"top10.csv\",index=False)\ncsv_file2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}